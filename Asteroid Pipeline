###This code (up to the feedforward neural network section) was made in collaboration with Guillermo Mazzilli, Carina Shanahan, and Can Goral 
#this is our final project for our Data Mining class, a core class in the Master's in Data Science program at Embry-Riddle. 


#Setup
# Import packages
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.base import BaseEstimator, TransformerMixin
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

# Read the merged_data.csv file from a public shared link

url = 'https://drive.google.com/file/d/12bXJAS5gN2gw0HNmY5Lf7AFusna27siC/view?usp=sharing'
url = 'https://drive.google.com/uc?id=' + url.split('/')[-2]
df = pd.read_csv(url)

# Show the dimensions of the dataset
df.shape

# Check if dataset is balanced or unbalanced

# Calculate the total number of records
total_records = len(df)

# Calculate the percentage of records for each class
percentage_Y = (df['pha'].value_counts()['Y'] / total_records) * 100
percentage_N = (df['pha'].value_counts()['N'] / total_records) * 100

# Print the percentages
print(f"Percentage of records with class 'Y': {percentage_Y:.2f}%")
print(f"Percentage of records with class 'N': {percentage_N:.2f}%")

# Check for NaN's

print(df.isnull().sum())

# Drop nan rows since there are very very few
df.dropna(inplace=True)
df.shape

#display the first five rows of the "tidy" dataset
df.head()

#display the data types
df.dtypes

df.drop(columns=['full_name','designation','des','orbit_id', 'h', 'number'], inplace=True)
df.head()

#Defining the classes
#Is a PHA = 1; Not a PHA = 0
df['PHA'] = df['pha'].map({'Y': 1, 'N': 0})
df.drop(columns='pha',inplace=True)
df.head()
                                                           ##INITIAL VISUALIZATIONS##
#Correlation between features
correlation_matrix = df.corr()

mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))

plt.figure(figsize = (10,8))
plt.rcParams.update({'font.size': 10})
sns.heatmap(correlation_matrix, cmap = "PiYG", vmin = -1, vmax = 1, center = 0, annot = True, fmt = '.2f', linewidths = .5, mask = mask)
plt.show()
                                                             ##SOME INTERESTING TRENDS:##
#The headmap above shows that the following pairs of attributes are highly correlated:
#v_rel with v_inf
#v_rel with e
#v_inf with e
#dist with dist_min
#dist with dist_max
#a with ad

#Due to the strong correlation between these and other attributes, and the relatively low correlation between these and the class labels, we will drop
#v_inf
#dist
#dist_min
#per_y
#ad

#splitting the dataset + more visualizations 
# Split the dataset into features (X) and target variable (y)
X = df.drop(columns=['PHA'])  # Features
y = df['PHA']  # Target variable

X.shape

list(X.columns)

# Show feature distribution
# create a box and whisker plot of each numerical attribute
fig = plt.figure(figsize = (7,7))
plt.boxplot(X, labels = list(X.columns))
plt.xticks(rotation=45)
plt.show()

# Split the dataset into training and testing subsets with stratified sampling
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)

# Print the proportions of classes in the training and testing subsets
print("Proportions of classes in the training subset:")
print(y_train.value_counts(normalize=True) * 100)
print("\nProportions of classes in the testing subset:")
print(y_test.value_counts(normalize=True) * 100)

from sklearn.preprocessing import StandardScaler

# Standardize features
bleh = StandardScaler().fit(X)

# Show feature distribution
# create a box and whisker plot of each numerical attribute
fig = plt.figure(figsize = (7,7))
plt.boxplot(bleh.transform(X), labels = list(X.columns))
plt.xticks(rotation=45)
plt.show()

from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC # added by Carina
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report
from sklearn.metrics import roc_auc_score
from sklearn.metrics import cohen_kappa_score #added by Austin
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
# -----------------------------------------------------------------------------#
# Create a pipeline for Logistic Regression
pipeline_lr = Pipeline([
    #('scaler', StandardScaler()),  # Standardize features
    ('clf', LogisticRegression())  # Logistic Regression model
])

# Define hyperparameters grid for Logistic Regression
param_grid_lr = {
    'clf__C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization parameter
    'clf__solver': ['liblinear', 'saga']  # Solver for optimization
}

# -----------------------------------------------------------------------------#
# Create a pipeline for Decision Tree
pipeline_dt = Pipeline([
    #('scaler', StandardScaler()),  # Standardize features
    ('clf', DecisionTreeClassifier())  # Decision Tree model
])

# Define hyperparameters grid for Decision Tree
param_grid_dt = {
    'clf__max_depth': [10,20,25],  # Maximum depth of the tree
    'clf__min_samples_split': [2,4,6,8,10],  # Minimum number of samples required to split a node
    'clf__min_samples_leaf': [2,4,6,8,10]  # Minimum number of samples required at each leaf node
}

# -----------------------------------------------------------------------------#
# Create a pipeline for Naive Bayes
pipeline_nb = Pipeline([
    #('scaler', StandardScaler()),  # Standardize features
    ('clf', GaussianNB())  # Naive Bayes model
])

# No hyperparameters to tune for Naive Bayes

# -----------------------------------------------------------------------------#

# SVC Model Removed from Pipeline for Manual Tuning

# -----------------------------------------------------------------------------#
# Create a pipeline for KNN
pipeline_knn = Pipeline([
    #('scaler', StandardScaler()),  # Standardize features
    ('clf', KNeighborsClassifier())  # knn model
])
# Define hyperparameters grid for KNN
param_grid_knn = {
    'clf__n_neighbors': [2,4,6,8,10], # Number of neighbors
    'clf__weights': [None, 'uniform','distance'] # Weights to try
}

# Create GridSearchCV objects for each model
grid_search_lr = GridSearchCV(pipeline_lr, param_grid_lr, cv=3, n_jobs=-1)
grid_search_dt = GridSearchCV(pipeline_dt, param_grid_dt, cv=3, n_jobs=-1)
grid_search_nb = GridSearchCV(pipeline_nb, {}, cv=3, n_jobs=-1)  # No hyperparameters for Naive Bayes
grid_search_knn = GridSearchCV(pipeline_knn, param_grid_knn, cv = 3, n_jobs=-1)

# List of models for iteration
models = [
    ('Naive Bayes', grid_search_nb),
    ('Logistic Regression', grid_search_lr),
    ('Decision Tree', grid_search_dt),
    ('K Nearest Neighbors', grid_search_knn)
]

                                                 ###EVALUATION###
# Train and evaluate each model using GridSearchCV
for name, model in models:
    print(f"Evaluating {name}...")
    model.fit(X_train, y_train)
    print("Best parameters:", model.best_params_)
    print("Train accuracy:", model.best_score_ * 100)
    print("Test accuracy:", model.score(X_test, y_test) * 100)
    print("Kappa Score :\n", cohen_kappa_score(y_test,model.predict(X_test)))
    print("AUC :\n", roc_auc_score(y_test,model.predict(X_test)))
    print("Precision Rate :\n", precision_score(y_test,model.predict(X_test)))
    print("Recall Rate :\n", recall_score(y_test,model.predict(X_test)))
    print("F-Score :\n", f1_score(y_test,model.predict(X_test)))
    print("Classification Report :\n", classification_report(y_test,model.predict(X_test)))
    print()

#The SVM Was Manually Tuned Outside the Pipeline
from sklearn.linear_model import SGDClassifier
svm_class_weights = {0: 1, 1: 8.283}
svm_clf = SGDClassifier(random_state=42, loss='hinge', penalty='l1', class_weight=svm_class_weights)

# Fit the classifier to the training data
svm_clf.fit(X_train, y_train)

# Predict the labels for the test data
y_ptrain_svm = svm_clf.predict(X_train)
y_pred_svm = svm_clf.predict(X_test)

# Evaluate the Model
svm_train = accuracy_score(y_train,y_ptrain_svm)
svm_accuracy = accuracy_score(y_test, y_pred_svm)
svm_precision = precision_score(y_test, y_pred_svm)
svm_recall = recall_score(y_test,y_pred_svm)
svm_f1 = f1_score(y_test, y_pred_svm)

#Display the results
print("Train Accuracy:",svm_train)
print("Test Accuracy:", svm_accuracy)
print("Precision:", svm_precision)
print("Recall:", svm_recall)
print("F1 Score:", svm_f1)
print("Kappa Score :\n", cohen_kappa_score(y_test,y_pred_svm))
print("AUC :\n", roc_auc_score(y_test,y_pred_svm))
print(classification_report(y_test, y_pred_svm))

# Get the Decision Tree's depth
max_depth = grid_search_dt.best_params_['clf__max_depth']

# Print the depth
print(f"Model depth: {max_depth}")





