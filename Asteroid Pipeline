###This code (up to the feedforward neural network section) was made in collaboration with Guillermo Mazzilli, Carina Shanahan, and Can Goral 
#this is our final project for our Data Mining class, a core class in the Master's in Data Science program at Embry-Riddle. 


#Setup
# Import packages
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.base import BaseEstimator, TransformerMixin
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

# Read the merged_data.csv file from a public shared link

url = 'https://drive.google.com/file/d/12bXJAS5gN2gw0HNmY5Lf7AFusna27siC/view?usp=sharing'
url = 'https://drive.google.com/uc?id=' + url.split('/')[-2]
df = pd.read_csv(url)

# Show the dimensions of the dataset
df.shape

# Check if dataset is balanced or unbalanced

# Calculate the total number of records
total_records = len(df)

# Calculate the percentage of records for each class
percentage_Y = (df['pha'].value_counts()['Y'] / total_records) * 100
percentage_N = (df['pha'].value_counts()['N'] / total_records) * 100

# Print the percentages
print(f"Percentage of records with class 'Y': {percentage_Y:.2f}%")
print(f"Percentage of records with class 'N': {percentage_N:.2f}%")

# Check for NaN's

print(df.isnull().sum())

# Drop nan rows since there are very very few
df.dropna(inplace=True)
df.shape

#display the first five rows of the "tidy" dataset
df.head()

#display the data types
df.dtypes

df.drop(columns=['full_name','designation','des','orbit_id', 'h', 'number'], inplace=True)
df.head()

#Defining the classes
#Is a PHA = 1; Not a PHA = 0
df['PHA'] = df['pha'].map({'Y': 1, 'N': 0})
df.drop(columns='pha',inplace=True)
df.head()
                                                           ##INITIAL VISUALIZATIONS##
#Correlation between features
correlation_matrix = df.corr()

mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))

plt.figure(figsize = (10,8))
plt.rcParams.update({'font.size': 10})
sns.heatmap(correlation_matrix, cmap = "PiYG", vmin = -1, vmax = 1, center = 0, annot = True, fmt = '.2f', linewidths = .5, mask = mask)
plt.show()
                                                             ##SOME INTERESTING TRENDS:##
#The headmap above shows that the following pairs of attributes are highly correlated:
#v_rel with v_inf
#v_rel with e
#v_inf with e
#dist with dist_min
#dist with dist_max
#a with ad

#Due to the strong correlation between these and other attributes, and the relatively low correlation between these and the class labels, we will drop
#v_inf
#dist
#dist_min
#per_y
#ad

#splitting the dataset + more visualizations 
# Split the dataset into features (X) and target variable (y)
X = df.drop(columns=['PHA'])  # Features
y = df['PHA']  # Target variable

X.shape

list(X.columns)

# Show feature distribution
# create a box and whisker plot of each numerical attribute
fig = plt.figure(figsize = (7,7))
plt.boxplot(X, labels = list(X.columns))
plt.xticks(rotation=45)
plt.show()

# Split the dataset into training and testing subsets with stratified sampling
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)

# Print the proportions of classes in the training and testing subsets
print("Proportions of classes in the training subset:")
print(y_train.value_counts(normalize=True) * 100)
print("\nProportions of classes in the testing subset:")
print(y_test.value_counts(normalize=True) * 100)

from sklearn.preprocessing import StandardScaler

# Standardize features
bleh = StandardScaler().fit(X)

# Show feature distribution
# create a box and whisker plot of each numerical attribute
fig = plt.figure(figsize = (7,7))
plt.boxplot(bleh.transform(X), labels = list(X.columns))
plt.xticks(rotation=45)
plt.show()

from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC # added by Carina
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report
from sklearn.metrics import roc_auc_score
from sklearn.metrics import cohen_kappa_score #added by Austin
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
# -----------------------------------------------------------------------------#
# Create a pipeline for Logistic Regression
pipeline_lr = Pipeline([
    #('scaler', StandardScaler()),  # Standardize features
    ('clf', LogisticRegression())  # Logistic Regression model
])

# Define hyperparameters grid for Logistic Regression
param_grid_lr = {
    'clf__C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization parameter
    'clf__solver': ['liblinear', 'saga']  # Solver for optimization
}

# -----------------------------------------------------------------------------#
# Create a pipeline for Decision Tree
pipeline_dt = Pipeline([
    #('scaler', StandardScaler()),  # Standardize features
    ('clf', DecisionTreeClassifier())  # Decision Tree model
])

# Define hyperparameters grid for Decision Tree
param_grid_dt = {
    'clf__max_depth': [10,20,25],  # Maximum depth of the tree
    'clf__min_samples_split': [2,4,6,8,10],  # Minimum number of samples required to split a node
    'clf__min_samples_leaf': [2,4,6,8,10]  # Minimum number of samples required at each leaf node
}

# -----------------------------------------------------------------------------#
# Create a pipeline for Naive Bayes
pipeline_nb = Pipeline([
    #('scaler', StandardScaler()),  # Standardize features
    ('clf', GaussianNB())  # Naive Bayes model
])

# No hyperparameters to tune for Naive Bayes

# -----------------------------------------------------------------------------#

# SVC Model Removed from Pipeline for Manual Tuning

# -----------------------------------------------------------------------------#
# Create a pipeline for KNN
pipeline_knn = Pipeline([
    #('scaler', StandardScaler()),  # Standardize features
    ('clf', KNeighborsClassifier())  # knn model
])
# Define hyperparameters grid for KNN
param_grid_knn = {
    'clf__n_neighbors': [2,4,6,8,10], # Number of neighbors
    'clf__weights': [None, 'uniform','distance'] # Weights to try
}

# Create GridSearchCV objects for each model
grid_search_lr = GridSearchCV(pipeline_lr, param_grid_lr, cv=3, n_jobs=-1)
grid_search_dt = GridSearchCV(pipeline_dt, param_grid_dt, cv=3, n_jobs=-1)
grid_search_nb = GridSearchCV(pipeline_nb, {}, cv=3, n_jobs=-1)  # No hyperparameters for Naive Bayes
grid_search_knn = GridSearchCV(pipeline_knn, param_grid_knn, cv = 3, n_jobs=-1)

# List of models for iteration
models = [
    ('Naive Bayes', grid_search_nb),
    ('Logistic Regression', grid_search_lr),
    ('Decision Tree', grid_search_dt),
    ('K Nearest Neighbors', grid_search_knn)
]

                                                 ###EVALUATION###
# Train and evaluate each model using GridSearchCV
for name, model in models:
    print(f"Evaluating {name}...")
    model.fit(X_train, y_train)
    print("Best parameters:", model.best_params_)
    print("Train accuracy:", model.best_score_ * 100)
    print("Test accuracy:", model.score(X_test, y_test) * 100)
    print("Kappa Score :\n", cohen_kappa_score(y_test,model.predict(X_test)))
    print("AUC :\n", roc_auc_score(y_test,model.predict(X_test)))
    print("Precision Rate :\n", precision_score(y_test,model.predict(X_test)))
    print("Recall Rate :\n", recall_score(y_test,model.predict(X_test)))
    print("F-Score :\n", f1_score(y_test,model.predict(X_test)))
    print("Classification Report :\n", classification_report(y_test,model.predict(X_test)))
    print()

#The SVM Was Manually Tuned Outside the Pipeline
from sklearn.linear_model import SGDClassifier
svm_class_weights = {0: 1, 1: 8.283}
svm_clf = SGDClassifier(random_state=42, loss='hinge', penalty='l1', class_weight=svm_class_weights)

# Fit the classifier to the training data
svm_clf.fit(X_train, y_train)

# Predict the labels for the test data
y_ptrain_svm = svm_clf.predict(X_train)
y_pred_svm = svm_clf.predict(X_test)

# Evaluate the Model
svm_train = accuracy_score(y_train,y_ptrain_svm)
svm_accuracy = accuracy_score(y_test, y_pred_svm)
svm_precision = precision_score(y_test, y_pred_svm)
svm_recall = recall_score(y_test,y_pred_svm)
svm_f1 = f1_score(y_test, y_pred_svm)

#Display the results
print("Train Accuracy:",svm_train)
print("Test Accuracy:", svm_accuracy)
print("Precision:", svm_precision)
print("Recall:", svm_recall)
print("F1 Score:", svm_f1)
print("Kappa Score :\n", cohen_kappa_score(y_test,y_pred_svm))
print("AUC :\n", roc_auc_score(y_test,y_pred_svm))
print(classification_report(y_test, y_pred_svm))

# Get the Decision Tree's depth
max_depth = grid_search_dt.best_params_['clf__max_depth']

# Print the depth
print(f"Model depth: {max_depth}")

                                                ###Ensemble methods###
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier

# -----------------------------------------------------------------------------#
# Create a pipeline for the Random Forest
pipeline_rf = Pipeline([
    #('scaler', StandardScaler()),  # Standardize features
    ('clf', RandomForestClassifier())  # Random Forest model
])

# Define hyperparameters grid for RF
param_grid_rf = {
    'clf__n_estimators': [100,120,150], # Number of estimators in the forest
    'clf__max_depth': [5,10,15,20, 35, 50] # Maximum depth of each tree
}


grid_search_rf = GridSearchCV(pipeline_rf, param_grid_rf, cv = 3, n_jobs=-1)

# List of models for iteration
models = [
    ('Random Forest', grid_search_rf),
]

# Train and evaluate each model using GridSearchCV
for name, model in models:
    print(f"Evaluating {name}...")
    model.fit(X_train, y_train)
    print("Best parameters:", model.best_params_)
    print("Train accuracy:", model.best_score_ * 100)
    print("Test accuracy:", model.score(X_test, y_test) * 100)
    print("Kappa Score :\n", cohen_kappa_score(y_test,model.predict(X_test)))
    print("AUC :\n", roc_auc_score(y_test,model.predict(X_test)))
    print("Precision Rate :\n", precision_score(y_test,model.predict(X_test)))
    print("Recall Rate :\n", recall_score(y_test,model.predict(X_test)))
    print("F-Score :\n", f1_score(y_test,model.predict(X_test)))
    print("Classification Report :\n", classification_report(y_test,model.predict(X_test)))
    print()

#feature importances 

importances = grid_search_rf.best_estimator_.named_steps['clf'].feature_importances_
features = X.columns
feature_importances = list(zip(features, importances))

#sorting the features in descending order
feature_importances.sort(key=lambda x: x[1], reverse=True)

#unzipping the sorted feature importances
sorted_features, sorted_importances = zip(*feature_importances)
plt.figure(figsize=(10, 6))
sns.barplot(x=np.array(sorted_importances), y=np.array(sorted_features), orient='h')
plt.xlabel('Importance Scores')
plt.ylabel('Features')
plt.title('Feature Importances for Random Forest')
plt.show()

#Confusion Matrices
from sklearn.metrics import confusion_matrix

#Decision Tree confusion matrix
cfm_dt = confusion_matrix(y_test, grid_search_dt.predict(X_test))

#KNN Confusion Matrix
cfm_knn= confusion_matrix(y_test, grid_search_knn.predict(X_test))
#Random Forest Confusion Matrix
cfm_rf = confusion_matrix(y_test, grid_search_rf.predict(X_test))

#Display the results
print("Confusion Matrices:\n")
print("Decision Tree:\n", cfm_dt)
print("KNN:\n", cfm_knn)
print("Random Forest:\n", cfm_rf)

#NEURAL NETWORK
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np

#PHA column is the target
X = df.drop('PHA', axis=1).values
y = df['PHA'].values

# Normalize the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

                                    ###FEEDFORWARD NEURAL NETWORK###
# Build the model: FEEDFORWARD NEURAL NETWORK

#define the model
model_large = Sequential()

#Starting with 128 neurons. 
#This number was chosen because we are working with big data
#The Dense layer receives the output from previous layers. Every neuron is connected to a neuron in the previous level

# relu = rectified linear unit activation function
model_large.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))
#Dropout used to avoid overfitting
model_large.add(Dropout(0.5))


model_large.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))

#32 neurons
model_large.add(Dense(32, activation='relu'))

#the sigmoid activation function is used for binary classification
model_large.add(Dense(1, activation='sigmoid'))

# Compile the model
model_large.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
history = model_large.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2) #50 epochs

#A plot to visualize training loss and validation loss
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

#EVALUATING THE MODEL
loss, accuracy = model_large.evaluate(X_test, y_test)
print(f'Test Accuracy: {accuracy:.2f}')

from sklearn.metrics import precision_score, recall_score, f1_score, cohen_kappa_score, roc_auc_score
from sklearn.metrics import roc_auc_score, roc_curve, auc
# Make predictions about the test set
y_pred_proba = model_large.predict(X_test)
y_pred = (y_pred_proba > 0.5).astype(int).flatten()
#y_test_classes = y_test.argmax(axis=1)
#y_pred_classes = y_pred.argmax(axis=1)

# Calculate precision, recall, and F1 score with average='weighted'
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')
kappa = cohen_kappa_score(y_test, y_pred)

#fpr = false positive rate
#tpr = true positive rate
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
AUC = auc(fpr, tpr)

#display the results
#print(f'AUC: {roc_auc:.2f}')
print(f"Precision: {precision}")
print(f"Recall Score: {recall}")
print(f"F1 Score: {f1}")
print(f"Kappa Score: {kappa}")
print(f"Area Under the Curve: {AUC}")

#PERMUTE/SHUFFLE FEATURES: FEATURE IMPORTANCE
original_loss, original_accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f'Original Model Accuracy: {original_accuracy:.4f}')
def permute_feature_importance(model, X_test, y_test, original_accuracy):
    feature_importances = []
    for col in range(X_test.shape[1]):
        X_test_permuted = X_test.copy()
        np.random.shuffle(X_test_permuted[:, col])  # Permute features

        # Evaluate the model with permuted data
        loss, accuracy = model.evaluate(X_test_permuted, y_test, verbose=0)

        # Calculate feature importance as the quotient of permuted error by original error
        feature_importance = accuracy / original_accuracy
        feature_importances.append((col, feature_importance))

    return feature_importances

# Calculate feature importances
feature_importances = permute_feature_importance(model, X_test, y_test, original_accuracy)

# Step 3: Sort features by descending feature importance
feature_importances.sort(key=lambda x: x[1], reverse=True)

# Display the feature importances
feature_names = df.drop('PHA', axis=1).columns
sorted_feature_importances = [(feature_names[col], importance) for col, importance in feature_importances]

print("Feature Importances (sorted):")
for feature, importance in sorted_feature_importances:
    print(f"{feature}: {importance:.4f}")

#VISUALING FEATURE IMPORTANCES
features, importances = zip(*sorted_feature_importances)
plt.figure(figsize=(8, 6))
#bar graph
plt.barh(features, importances, color='forestgreen')
plt.xlabel('Importance Score')
plt.ylabel('Features')
plt.title('Feature Importances of Asteroid Parameters for a Feedforward Neural Network')
plt.gca().invert_yaxis()  # Invert y-axis to have the most important feature at the top
plt.show()








